---
title: "practial_4"
author: "Tsz Kin, Siu"
date: "2024-06-10"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r libraries, echo=FALSE}
# Using the same library we used earlier in the course for tabular data because we know it works!
library(xgboost)

# EEG manipulation library in R (although very limited compared to signal processing libraries available in other languages, matlab might actually still be a leader in this specific area)
library(eegkit)

# some time series functions (that we only skim the depths of)
library(forecast)
library(tseries)
library(caret)

# just tidyverse libraries that should already be installed
library(dplyr)
library(reshape2)
library(purrr)
library(ggplot2)
```


###   EEG Eye Detection Data


```{r parse_data}
eeg_url <- "https://h2o-public-test-data.s3.amazonaws.com/smalldata/eeg/eeg_eyestate_splits.csv"
eeg_data <- read.csv(eeg_url)

# add timestamp
Fs <- 117 / nrow(eeg_data)
eeg_data <- transform(eeg_data, ds = seq(0, 116.99999, by = Fs), eyeDetection = as.factor(eyeDetection))

### eye status class distribution for the full dataset
print(table(eeg_data$eyeDetection))

# split dataset into train, validation, test
eeg_train <- subset(eeg_data, split == 'train', select = -split)

### eye status class distribution for the training dataset 
print(table(eeg_train$eyeDetection))

eeg_validate <- subset(eeg_data, split == 'valid', select = -split)
eeg_test <- subset(eeg_data, split == 'test', select = -split)
```



> Q0) Knowing the `eeg_data` contains 117 seconds of data, inspect the `eeg_data` dataframe and the code above to determine how many samples per second were taken?


```{r q0}
print(paste0("Time interval between measurements (in seconds) = ", Fs))
print(paste0("Time interval between measurements (in milliseconds) = ", Fs*1000))
print(paste0("Frequency of measurements (Number of samples taken per second) = ", 1/Fs))
```

> Q1) How many EEG electrodes/sensors were used?


```{r q1}
print(paste0("Number of EEG electrodes = ", length(colnames(eeg_data)[!colnames(eeg_data) %in% c("eyeDetection","split","ds")])))
```


###  Exploratory Data Analysis


```{r check_na}
#  check number of missing values in the dataset63
sum(is.na(eeg_data))
```

```{r plot_data, cache=TRUE, fig.width=12, fig.height=6}
#  transform the data into a long format
melt <- reshape2::melt(eeg_data %>% dplyr::select(-split), id.vars=c("eyeDetection", "ds"), variable.name = "Electrode", value.name = "microvolts")

#  plot the 'electrode intensities per sampling time'
ggplot2::ggplot(melt, ggplot2::aes(x=ds, y=microvolts, color=Electrode)) + 
  ggplot2::geom_line() + 
  ggplot2::ylim(3500,5000) + 
  ggplot2::geom_vline(ggplot2::aes(xintercept=ds), data=dplyr::filter(melt, eyeDetection==0), alpha=0.005)
```


> Q2) Do you see any obvious patterns between eyes being open (dark grey blocks in the plot) and the EEG intensities?

At the instant When eyes are being open, the EEG intensities measured by micro-volts had shown a reduction of varying extents (slight, moderate, large) by electrodes.
This is indicated by the sharp downward peaks appeared at the leftmost vertical lines (or edges) of the darker grey colored blocks in the plot.
Then, within a short time lapse, the intensities bounced back to the steady level as similar to the time intervals when eyes are closed with a relatively flat trend and a seasonal pattern.


> Q3) Similarly, based on the distribution of eye open/close state over time to anticipate any temporal correlation between these states?

From the plot, when there was a transition between the open and close of the eyes, the EEG intensities would record an opposite change pattern.
When eyes change from open to closed, the EEG intensities increased drastically for most electrodes, hence indicating a strong positive temporal relationship.
When eyes change from closed to open, EEG signals from several specific electrodes, e.g., F4, F8, and AF4, decreased more significantly but overall the magnitudes of changes were smaller than transition from open to closed. 
Generally, this illustrates a moderate negative temporal relationship. 
When transition does not taken place at the time step, it tends to have a steady state.



```{r compare_distrib}
#  transform the training data into a long format
melt_train <- reshape2::melt(eeg_train, id.vars=c("eyeDetection", "ds"), variable.name = "Electrode", value.name = "microvolts")

# filter huge outliers in voltage
filt_melt_train <- dplyr::filter(melt_train, microvolts %in% (3750:5000)) %>% dplyr::mutate(eyeDetection=as.factor(eyeDetection))

#  plot the 'electrode intensities by EEG electrode/sensor and eye status'
ggplot2::ggplot(filt_melt_train, ggplot2::aes(y=Electrode, x=microvolts, fill=eyeDetection)) + ggplot2::geom_boxplot()
```


```{r compare_summary_stats}
##  table of summary statistics by EEG electrode and eye status
filt_melt_train %>% dplyr::group_by(eyeDetection, Electrode) %>% 
    dplyr::summarise(mean = mean(microvolts), median=median(microvolts), sd=sd(microvolts)) %>% 
    dplyr::arrange(Electrode)
```


> Q4) Based on these analyses are any electrodes consistently more intense or varied when eyes are open?

Based on the box-whisker diagram, electrodes AF4, FC6, and AF3 have equal medians when eyes are open and closed, but their upper quartiles are higher when eyes are open. This can be verified by their higher means when eyes are open,
Electrode F8 has a significantly higher median when eyes are open. Based on the summary statistics table, This is also true for the electrode T8, the median of intensity increases by 20 when eyes are open, but it is less clear in the plot.
In contrast, the median EEG intensities of electrodes O1 and F7 decreases when eyes are open.



###  Time-Related Trends


```{r convert_to_tseries}
#  apply Augmented Dickey-Fuller Test to check the stationarity of time series
apply(eeg_train, 2, tseries::adf.test)
```


> Q5)  What is stationarity?

Stationarity in time series means the statistical properties of the observations across the time steps does not depend on the time dimension.
This suggests that the parameters of probabilistic distributions of the observations, such as mean and variance, remain constant over time.  


> Q6)  Why are we interested in stationarity? What do the results of these tests tell us? (ignoring the lack of multiple comparison correction...)

Stationarity is an important assumption for different statistical models applied on time series analysis and forecasting, such as ARIMA and GARCHS. 
If the statistical parameters differ with time (i.e., non-stationary), the process becomes stochastic and complex that is hard to model. 
This makes pattern recognition and interpretation like trends and seasonality unreliable or inaccurate.

Augmented Dickey-Fuller (ADF) Test is a statistical test of time series stationarity. 
The above outputs are the ADF Tests applied on each feature of the EEG training dataset.  
Omitting the last two variables (eyeDetection, ds), all p-values of the tests on EEG signals of the 14 electrodes are smaller than 0.05 (statistically significant at 5% level).
As the null hypothesis of ADF test is non-stationarity of the time series. That means, the results indicate all 14 EEG intensity time series are stationary. 



```{r correlation, fig.width=40, fig.height=40}
#  check auto-correlation plot
forecast::ggAcf(eeg_train %>% dplyr::select(-ds))
```



> Q7) Do any fields show signs of strong autocorrelation (diagonal plots)? Do any pairs of fields show signs of cross-correlation? Provide examples.

The following electrodes (9 out of 14) show signs of strong autocorrelation:  F7, F3, FC5, T7, O1, O2, T8, FC6, F4.

The magnitudes of autocorrelation across the 30-lag window for these features ranged from around 0.10 to 0.50, exceeding the confidence interval of significance. 

For the pairwise correlations between the electrodes, 

i)  Relatively strong / more significant correlations are found in pairs involving any of the followings:  F3, FC5, T7, O1, O2, T8, FC6, F4.

ii) Relatively strong / more significant correlations are found specifically between F7 and F3, FC5, T7, O1.

iii) Weak / marginally significant correlations are found specifically between F7 and O2, T8, FC6, F4.



###  Frequency-Space

```{r fft_open}
#  plot of power spectral density when eyes are open
eegkit::eegpsd(eeg_train %>% dplyr::filter(eyeDetection == 0) %>% dplyr::select(-eyeDetection, -ds), Fs = Fs, xlab="Eye Open")
```


```{r fft_closed}
#  plot of power spectral density when eyes are closed
eegkit::eegpsd(eeg_train %>% dplyr::filter(eyeDetection == 1) %>% dplyr::select(-eyeDetection, -ds), Fs = Fs, xlab="Eye Closed")
```



> Q8)  Do you see any differences between the power spectral densities for the two eye states? If so, describe them.

When eyes are open, channels 6 (P7) and 14 (AF4) were observed to have high power intensities, while other electrodes had a relatively small variation, with channel 4 (FC5) and 11 (FC6) having relatively lower power levels.

When eyes are closed, channels 1 (AF3), 9 (P8) and 13 (F8) had the highest power intensities, while channel 8 (O2) and 10 (F4) had significantly lower power levels indicated by a deeper blue colour that did not appear when eyes are open.

Moreover, channel 6 (P7) and 14 (AF4) showed the largest variation between the two eye status states, changing from among the highest powers when eyes are open to among the lowest powers when eyes are closed.



###  Independent Component Analysis


```{r ica, warning=FALSE, fig.width=12}
#  apply Independent Component Analysis to the training dataset
ica <- eegkit::eegica(eeg_train %>% dplyr::select(-eyeDetection, -ds), nc=3, method='fast', type='time')
#  assigned de-correlated component
mix <- dplyr::as_tibble(ica$M)
mix$eyeDetection <- eeg_train$eyeDetection
mix$ds <- eeg_train$ds

mix_melt <- reshape2::melt(mix, id.vars=c("eyeDetection", "ds"), variable.name = "Independent Component", value.name = "M")

# plot de-correlated component by time interval
ggplot2::ggplot(mix_melt, ggplot2::aes(x=ds, y=M, color=`Independent Component`)) + 
  ggplot2::geom_line() + 
  ggplot2::geom_vline(ggplot2::aes(xintercept=ds), data=dplyr::filter(mix_melt, eyeDetection==0), alpha=0.005) +
  ggplot2::scale_y_log10()
```


> Q9) Does this suggest eye opening relates to an independent component of activity across the electrodes?

From the plot, it appears that the mixture of signals from the electrodes are predominantly attributed to or explained by only the 1st independent component. 
It suggests that the time series patterns of eye opening captured across all electrodes is related to the same source of activity, all electrode measurements are related to each other.



##  Eye Opening Prediction


```{r xgboost, cache=TRUE}
# Convert the training and validation datasets to matrices
eeg_train_matrix <- as.matrix(dplyr::select(eeg_train, -eyeDetection, -ds))
eeg_train_labels <- as.numeric(eeg_train$eyeDetection) -1

eeg_validate_matrix <- as.matrix(dplyr::select(eeg_validate, -eyeDetection, -ds))
eeg_validate_labels <- as.numeric(eeg_validate$eyeDetection) -1

# Build the xgboost model
model <- xgboost(data = eeg_train_matrix, 
                 label = eeg_train_labels,
                 nrounds = 100,
                 max_depth = 4,
                 eta = 0.1,
                 objective = "binary:logistic")
```


```{r xgboost_result}
#  training loss of xgboost
print(model)
```

> Q10)  Using the caret library (or any other library/model type you want such as a naive Bayes) fit another model to predict eye opening.

```{r model2, cache=TRUE}
# fitting cross-validation parameters
naive_bayes_grid = expand.grid(laplace = seq(0, 1, 0.1), usekernel=TRUE, adjust = seq(0.5, 3.0, 0.5))
fit.control <- trainControl(method="cv", number=5, classProbs = TRUE, summaryFunction = twoClassSummary)

# Build the Naive Bayes model
model2 <- caret::train(x = eeg_train_matrix, 
                       y = ifelse(eeg_train_labels == 1, "Closed", "Open"), 
                       metric = "ROC", 
                       method = "naive_bayes",
                       tuneGrid = naive_bayes_grid, 
                       trControl = fit.control)
```

> Q11)  Using the best performing of the two models (on the validation dataset) calculate and report the test performance.

From the below results on the validation dataset, XGBoost outperforms Naive Bayes.
Using XGBoost for evaluating the testing dataset, it appears that the the predictive performance is robust.

```{r testing_set}
#  Convert the testing datasets to matrices
eeg_test_matrix <- as.matrix(dplyr::select(eeg_test, -eyeDetection, -ds))
eeg_test_labels <- as.numeric(eeg_test$eyeDetection) -1
```

```{r model_prediction}
#  xgboost prediction on validation and testing sets
xgb.pred.validate <- predict(model, newdata=eeg_validate_matrix)
xgb.pred.test <- predict(model, newdata=eeg_test_matrix)

#  naive bayes prediction on validation and testing sets
nb.pred.validate <- predict(model2, newdata=eeg_validate_matrix)
nb.pred.test <- predict(model2, newdata=eeg_test_matrix)
```

```{r model_evaluation}
#  convert xgboost predictions into binary indicators
xgb.pred.validate.labels = ifelse(xgb.pred.validate > 0.5, 1, 0)
xgb.pred.test.labels = ifelse(xgb.pred.test > 0.5, 1, 0)

#  factorize the labels
eeg_validate_labels_nb = as.factor(ifelse(eeg_validate_labels  == 1, "Closed", "Open"))
eeg_test_labels_nb = as.factor(ifelse(eeg_test_labels  == 1, "Closed", "Open"))

cat(paste0("Validation performance of XGBoost :", "\n\n"))
confusionMatrix(as.factor(xgb.pred.validate.labels), as.factor(eeg_validate_labels))

cat("\n\n")

cat(paste0("Validation performance of Naive Bayes :", "\n\n"))
confusionMatrix(as.factor(nb.pred.validate), as.factor(eeg_validate_labels_nb), positive = "Open")
```

```{r model_testing}
cat(paste0("Testing performance of XGBoost :", "\n\n"))
confusionMatrix(as.factor(xgb.pred.test.labels), as.factor(eeg_test_labels))
```


> Q12)  Describe 2 possible alternative modeling approaches for prediction of eye opening from EEGs we discussed in the lecture but haven’t explored in this notebook.

Alternative Approach 1):

Gaussian Process model as described in the EEG lecture can be used to make predictions to the classification of eye opening given the multivariate signal time series data. It is a non-parametric model with infinite parameters. 
Adopting a Bayesian modelling concept, it is based on a Gaussian probability distribution prior, and finding the posterior distribution of parameters from possible functions that maximizes the likelihoods on the observed data.
Different types of kernel can be specified to represent the covariance relationship or structure in the data, like linear, polynomial, exponential, radial basis function, etc.

Alternative Approach 2):

More modern approaches for modelling the EEG data include deep learning using different neural network models. 
As mentioned in the epilepsy seizure case study, neural network can be used to extract latent embeddings from multivariate time series features, with state-of-the-art attention-based mechanism (i.e., the family of Transformer models).
Besides the recurrent neural network (RNN) learning the sequential dependence, convolutional neural network (CNN) components can also be used when stacking the power spectral density features in terms of images.
The final output can be activated with a sigmoid function to complete the classification task of this objective for eye opening status predictions.


> Q13)  What are 2 R libraries you could use to implement these approaches? (note: you don’t actually have to implement them though!)

Gaussian Process classifier is available in the "caret" package.
It has dependency for implementation through the "kernlab" package.

Neural network or deep learning has different implementation frameworks, for example, Torch ("torch"), Tensorflow and Keras ("tensorflow" and "keras"), MXNet ("mxnet"), etc.
In the "caret" package, Multi-layer Perceptron Network (MLP) models with different algorithms are also available, which are implemented with dependencies like "neuralnet", "nnet", "deepnet", etc.




> Q14)  (Optional) As this is the last practical of the course - let me know how you would change future offerings of this course.

What worked and didn’t work for you (e.g., in terms of the practicals, tutorials, and lectures)?

**  The literature sessions are good, I think other than oral discussions, the learning experience will be more impressive, if some of the in-depth and inspiring discussed problems were documented and summarised on the course website, or if some supplementary resources about the solutions to overcome the limitations or shortcoming of the research mentioned in class can be provided.


Was learning how to run the practicals on your own machines instead of a clean server that will disappear after the course worth the technical challenges?

**  The practicals are fine for students with prior computer science or data science experience, but may be a bit over-challenging for students from other disciplines with a short and intensive course schedule.
    Running practicals in own machines will work better for students with less experience. For deep learning tasks, like MobileNet in practical 3, it may be challenging if only CPU is used. 
    Maybe a server with GPU resources for running that task can be beneficial, or a guide on using online computing resources may help.


What would you add or remove from the course?

**  The contents and learning components are good for me.


What was the main thing you will take away from this course?

**  It helps me to understand when machine learning may not work in health research or operational settings, and what are the prior steps we should investigate or understand before jumping into the fancy stuffs.
    Also, it is a good learning experience to think about conceiving a project when working on the proposal.


